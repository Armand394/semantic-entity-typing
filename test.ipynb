{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba1a934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Using cached transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence_transformers) (2.6.0)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.15.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB ? eta 0:00:00\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Using cached huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\antoi\\appdata\\roaming\\python\\python311\\site-packages (from sentence_transformers) (4.13.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\antoi\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\antoi\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.1.3)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB ? eta 0:00:00\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n",
      "Using cached sentence_transformers-4.0.2-py3-none-any.whl (340 kB)\n",
      "Using cached huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
      "Using cached transformers-4.51.0-py3-none-any.whl (10.4 MB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.1 MB 14.9 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.3/11.1 MB 16.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.9/11.1 MB 15.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.5/11.1 MB 14.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.3/11.1 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.5/11.1 MB 16.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.1 MB 19.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.1/11.1 MB 20.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.7/11.1 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.0/11.1 MB 17.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.8/11.1 MB 17.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.1/11.1 MB 16.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.6/11.1 MB 16.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.0/11.1 MB 16.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.7/11.1 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 12.8 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.2-cp311-cp311-win_amd64.whl (41.2 MB)\n",
      "   ---------------------------------------- 0.0/41.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/41.2 MB 33.4 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 1.4/41.2 MB 17.2 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.7/41.2 MB 11.9 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.8/41.2 MB 9.8 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 1.8/41.2 MB 9.8 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 1.8/41.2 MB 9.8 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 2.0/41.2 MB 6.8 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.8/41.2 MB 7.5 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 4.9/41.2 MB 11.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 6.8/41.2 MB 14.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 8.9/41.2 MB 17.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 10.9/41.2 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 13.4/41.2 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 15.8/41.2 MB 46.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 17.8/41.2 MB 46.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 19.3/41.2 MB 43.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 20.3/41.2 MB 40.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 22.1/41.2 MB 38.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 24.3/41.2 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 26.1/41.2 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 28.4/41.2 MB 38.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 30.7/41.2 MB 46.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 33.1/41.2 MB 46.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 34.5/41.2 MB 46.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 37.3/41.2 MB 50.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.9/41.2 MB 46.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.2 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.2 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.2 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.2 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.2/41.2 MB 27.3 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "   ---------------------------------------- 0.0/162.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 162.0/162.0 kB 9.5 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "   ---------------------------------------- 0.0/274.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 274.1/274.1 kB 16.5 MB/s eta 0:00:00\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: threadpoolctl, scipy, safetensors, regex, pyyaml, joblib, scikit-learn, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed huggingface-hub-0.30.1 joblib-1.4.2 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 sentence_transformers-4.0.2 threadpoolctl-3.6.0 tokenizers-0.21.1 transformers-4.51.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script huggingface-cli.exe is installed in 'c:\\Users\\antoi\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'c:\\Users\\antoi\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81fc9c8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_empty_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodule\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msampling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmhsk_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\OneDrive\\Documents\\Universite\\M1\\S2\\PLDAC\\semantic-entity-typing\\sampling\\mhsk_utils.py:16\u001b[39m\n\u001b[32m     14\u001b[39m     model = SentenceTransformer(\u001b[33m'\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m, device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Forces GPU\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall-MiniLM-L6-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_coherence_statsdf\u001b[39m(e_coherence, entity_labels):\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Create a reverse mapping from label + description back to ID\u001b[39;00m\n\u001b[32m     21\u001b[39m     reverse_entity_labels = {\n\u001b[32m     22\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv.get(\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39mk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv.get(\u001b[33m'\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m: k \n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m entity_labels.items()\n\u001b[32m     24\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:309\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[32m    303\u001b[39m     model_name_or_path,\n\u001b[32m    304\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    308\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    322\u001b[39m         model_name_or_path,\n\u001b[32m    323\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         config_kwargs=config_kwargs,\n\u001b[32m    331\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1802\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   1799\u001b[39m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[32m   1800\u001b[39m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[32m   1801\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1802\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1804\u001b[39m     module = module_class.load(model_name_or_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:81\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     78\u001b[39m     config_args = {}\n\u001b[32m     80\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     84\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:181\u001b[39m, in \u001b[36mTransformer._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28mself\u001b[39m.auto_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_peft_model:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_peft_model(model_name_or_path, config, cache_dir, **model_args, **adapter_only_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:4333\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4330\u001b[39m config.name_or_path = pretrained_model_name_or_path\n\u001b[32m   4332\u001b[39m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4333\u001b[39m model_init_context = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_init_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_is_ds_init_called\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4335\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33m_attn_implementation_autoset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\antoi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3736\u001b[39m, in \u001b[36mPreTrainedModel.get_init_context\u001b[39m\u001b[34m(cls, is_quantized, _is_ds_init_called)\u001b[39m\n\u001b[32m   3734\u001b[39m         init_contexts.append(set_quantized_state())\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     init_contexts = [no_init_weights(), \u001b[43minit_empty_weights\u001b[49m()]\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m init_contexts\n",
      "\u001b[31mNameError\u001b[39m: name 'init_empty_weights' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from module.utils import *\n",
    "from sampling.mhsk_utils import *\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')  # Forces GPU\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Specify locations for loading and saving data\n",
    "project_folder = os.getcwd()\n",
    "data_path = os.path.join(project_folder, \"data\", \"FB15kET_sample\")\n",
    "result_folder = os.path.join(project_folder, \"data_entity_metrics\")\n",
    "\n",
    "# Load Ids and descriptions\n",
    "e2id = read_id(os.path.join(data_path, 'entities.tsv'))\n",
    "r2id = read_id(os.path.join(data_path, 'relations.tsv'))\n",
    "t2id = read_id(os.path.join(data_path, 'types.tsv'))\n",
    "c2id = read_id(os.path.join(data_path, 'clusters.tsv'))\n",
    "\n",
    "e2desc, e2text = read_entity_wiki(os.path.join(data_path, 'entity_wiki.json'), e2id, 'hybrid')\n",
    "r2text = read_rel_context(os.path.join(data_path, 'relation2text.txt'), r2id)\n",
    "t2desc = read_type_context(os.path.join(data_path, 'hier_type_desc.txt'), t2id)\n",
    "\n",
    "# Load KG data\n",
    "df_triples = pd.read_csv(os.path.join(data_path, \"KG_train.txt\"), sep='\\t', header=None)\n",
    "df_train = pd.read_csv(os.path.join(data_path, \"ET_train.txt\"), sep='\\t', header=None)\n",
    "\n",
    "with open(os.path.join(data_path, 'entity_wiki.json'), \"r\") as f:\n",
    "    entity_labels = json.load(f)\n",
    "\n",
    "# Recompute coherence metrics for entities\n",
    "if not os.path.exists(os.path.join(result_folder, \"entity_metrics_sample.csv\")):\n",
    "    recompute_similarity(df_triples, df_train, r2text, r2id, e2desc, e2id, t2desc, t2id, result_folder)\n",
    "\n",
    "# Load entity coherence metrics\n",
    "e_coherence = pd.read_csv(os.path.join(result_folder, \"entity_metrics_sample.csv\"))\n",
    "\n",
    "# Entities with degree considered too low\n",
    "lowerb_quantiles = e_coherence[['kg_degree', 'et_degree']].quantile(0.1)\n",
    "low_kg_degree = lowerb_quantiles.loc['kg_degree']\n",
    "low_et_degree = lowerb_quantiles.loc['et_degree']\n",
    "\n",
    "# Filter only entities with bad metrics\n",
    "entity_low_degree_df = e_coherence[(e_coherence[\"kg_degree\"] <= low_kg_degree) \\\n",
    "                              & (e_coherence[\"et_degree\"] <= lowerb_quantiles.loc['et_degree'])]\n",
    "entity_low_degree = entity_low_degree_df['entity'].to_list()\n",
    "\n",
    "entity_kg_2hop = []\n",
    "entity_et_2hop = []\n",
    "for entity in tqdm(entity_low_degree, total=len(entity_low_degree), desc=\"Processing entities\", unit=\"entity\"):\n",
    "    \n",
    "    # Current sentences\n",
    "    kg_entity_text, _ = kg_sentences(df_triples, entity, r2text, r2id, e2desc, e2id)\n",
    "    et_train_sentences, _ = et_sentences(df_train, entity, t2desc, t2id)\n",
    "    entity_sentences = kg_entity_text + et_train_sentences\n",
    "\n",
    "    # Number 2-hop relations and types added\n",
    "    n_rel = int(round(low_kg_degree * (1 - len(kg_entity_text) / (low_kg_degree+1))) + 2)\n",
    "    n_type = int(round(low_et_degree*2* (1 - len(et_train_sentences) / (low_et_degree+1))) + 2)\n",
    "    \n",
    "    # 2-hop neighbor sentences\n",
    "    kg_2hop, kg_2hop_sentences = two_hop_neighbors(df_triples, entity, r2text, r2id, e2desc, e2id)\n",
    "    types_2hop, et_txt_2hop = two_hop_types(df_triples, df_train, entity, t2desc, t2id)\n",
    "\n",
    "    # 2-hop kg neighbor and type with highest average similarity score\n",
    "    kg_top_2hop = max_sim_2hop(entity_sentences, kg_2hop_sentences, kg_2hop, n_rel)\n",
    "    et_top_2hop = max_sim_2hop(entity_sentences, et_txt_2hop, types_2hop, n_type, kg=False)\n",
    "\n",
    "    # Store best results for additional information\n",
    "    for relation, entity2, direction in kg_top_2hop:\n",
    "        if direction == '-':\n",
    "            entity_kg_2hop.append((entity, relation, entity2))\n",
    "        elif direction == 'inv':\n",
    "            entity_kg_2hop.append((entity2, relation, entity))\n",
    "\n",
    "    save_entity_kg_2hop(entity_kg_2hop, os.path.join(data_path, 'relation2hop.tsv'))\n",
    "\n",
    "    for type in et_top_2hop:\n",
    "        entity_et_2hop.append((entity, type))\n",
    "\n",
    "\n",
    "# Entities with degree considered too high\n",
    "upperb_quantiles = e_coherence[['kg_degree', 'et_degree']].quantile(0.90)\n",
    "high_kg_degree = upperb_quantiles.loc['kg_degree']\n",
    "high_et_degree = upperb_quantiles.loc['et_degree']\n",
    "\n",
    "# Filter only entities with bad metrics\n",
    "entity_high_degree_df = e_coherence[(e_coherence[\"kg_degree\"] > high_kg_degree) \\\n",
    "                              & (e_coherence[\"et_degree\"] > high_et_degree)]\n",
    "entity_high_degree = entity_high_degree_df['entity'].to_list()\n",
    "\n",
    "# removed results dataframe\n",
    "kg_train_removed_df = pd.DataFrame()\n",
    "et_train_removed_df = pd.DataFrame()\n",
    "\n",
    "for entity in tqdm(entity_high_degree, total=len(entity_high_degree), desc=\"Processing entities\", unit=\"entity\"):\n",
    "\n",
    "    # Current sentences\n",
    "    kg_entity_text, neighbors = kg_sentences(df_triples, entity, r2text, r2id, e2desc, e2id, filter=False)\n",
    "    et_train_sentences, et_train = et_sentences(df_train, entity, t2desc, t2id)\n",
    "    entity_sentences = kg_entity_text + et_train_sentences\n",
    "\n",
    "    # Remove noisy relationships and types\n",
    "    n_kg_remove = int(math.ceil(len(kg_entity_text)*0.1))\n",
    "    n_et_remove = int(math.ceil(len(et_train_sentences)*0.1))\n",
    "\n",
    "    # Remove noisy neighbors through similarity score\n",
    "    kg_train_removed, et_train_removed = remove_noisy_neighbors(kg_entity_text, neighbors, et_train_sentences, et_train, n_kg_remove, n_et_remove)\n",
    "\n",
    "    # Store removed results\n",
    "    kg_train_removed_df = pd.concat([kg_train_removed_df, kg_train_removed], axis=0).reset_index(drop=True)\n",
    "    et_train_removed_df = pd.concat([et_train_removed_df, et_train_removed], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Update KG_train and ET_train without noise relations\n",
    "kg_train_new = df_triples.merge(kg_train_removed_df, on=[0, 1, 2], how='left', indicator=True)\n",
    "kg_train_new = kg_train_new[kg_train_new['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "et_train_new = df_train.merge(et_train_removed_df, how='left', indicator=True)\n",
    "et_train_new = et_train_new[et_train_new['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "# Convert 2-hop additions in dataframe\n",
    "kg_train_2hop = pd.DataFrame(entity_kg_2hop, columns=[0,1,2])\n",
    "et_train_2hop = pd.DataFrame(entity_et_2hop, columns=[0,1])\n",
    "\n",
    "# Final processed train files\n",
    "kg_train_processed = pd.concat([kg_train_new, kg_train_2hop], axis=0).reset_index(drop=True)\n",
    "et_train_processed = pd.concat([et_train_new, et_train_2hop], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Save files\n",
    "data_sample_dir_2hop = os.path.join(project_folder, \"data\", f\"FB15kET_sample_2hop\")\n",
    "os.makedirs(data_sample_dir_2hop, exist_ok=True)\n",
    "kg_train_processed.to_csv(os.path.join(data_sample_dir_2hop, \"KG_train.txt\"), sep='\\t', header=None, index=False)\n",
    "et_train_processed.to_csv(os.path.join(data_sample_dir_2hop, \"ET_train.txt\"), sep='\\t', header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "472f203f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\antoi\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\antoi\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\antoi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\antoi\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e3bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
